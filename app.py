# app.py

import streamlit as st
import os
import pandas as pd
import numpy as np
# Google GenAI'yi sadece Generation adÄ±mÄ± iÃ§in kullanÄ±yoruz
from google import genai 
# Embedding (VektÃ¶rleme) ve KosinÃ¼s BenzerliÄŸi iÃ§in
from sentence_transformers import SentenceTransformer
from scipy.spatial.distance import cosine 
from typing import List

# --- 1. SABÄ°T TANIMLAMALAR ---
EMBEDDING_MODEL_NAME = "paraphrase-multilingual-mpnet-base-v2"
GENERATION_MODEL = "gemini-2.5-pro" 
TOP_K_DOCUMENTS = 2 # Retrieval adÄ±mÄ±nda Ã§ekilecek en alakalÄ± dokÃ¼man sayÄ±sÄ±

# --- 2. STREAMLIT PERFORMANS OPTÄ°MÄ°ZASYONU ---
@st.cache_resource
def load_resources():
    st.info("Kaynaklar yÃ¼kleniyor: Veri seti ve Embedding Modeli...")
    
    # Veri setini yÃ¼kleme (Veri dosyanÄ±zÄ±n app.py ile aynÄ± dizinde olmasÄ± gerekir)
    try:
        df = pd.read_csv("erasmus_dataset.csv") 
    except FileNotFoundError:
        st.error("HATA: 'erasmus_dataset.csv' dosyasÄ± bulunamadÄ±. LÃ¼tfen GitHub reponuza ekleyin.")
        return None, None
    
    # Embedding Modelini yÃ¼kleme
    embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)
    
    # Embedding'leri oluÅŸturma
    documents = df['cevap'].tolist()
    st.info(f"Toplam {len(documents)} dokÃ¼man iÃ§in embedding hesaplanÄ±yor...")
    document_embeddings = embedding_model.encode(documents)
    df['embedding'] = list(document_embeddings)

    st.success("Kaynaklar baÅŸarÄ±yla yÃ¼klendi!")
    return df, embedding_model

# --- 3. RAG FONKSÄ°YONLARI ---

def find_most_relevant_document(query: str, df: pd.DataFrame, embedding_model, top_k: int) -> str:
    """
    Soru iÃ§in embedding oluÅŸturur ve en benzer dokÃ¼manlarÄ± bulur.
    """
    
    # Sorunun embedding'ini Sentence Transformer ile oluÅŸturma
    query_embedding = embedding_model.encode(query)

    # KosinÃ¼s benzerliÄŸi hesaplama: 1 - KosinÃ¼s UzaklÄ±ÄŸÄ±
    similarities = [1 - cosine(query_embedding, doc_embedding) for doc_embedding in df['embedding']]
    df['similarity'] = similarities
    
    # En yÃ¼ksek benzerliÄŸe sahip ilk k dokÃ¼manÄ± sÄ±ralama ve alma
    relevant_docs = df.sort_values(by='similarity', ascending=False).head(top_k)
    
    # Sadece ilgili cevap metinlerini tek bir metin (context) olarak birleÅŸtirme
    context = "\n\n---\n\n".join(relevant_docs['cevap'].tolist())
    return context


def generate_rag_answer(query: str, context: str) -> str:
    """
    Context ve soru ile Gemini modelinden cevap Ã¼retir.
    """
    
    # API AnahtarÄ±nÄ± Streamlit Secrets'tan gÃ¼venli bir ÅŸekilde oku
    try:
        # Streamlit Secrets'a eriÅŸimin en standart yolu budur.
        api_key = st.secrets["GEMINI_API_KEY"] 
        os.environ['GEMINI_API_KEY'] = api_key
        client = genai.Client()
    except KeyError:
        st.error("API AnahtarÄ± hatasÄ±: LÃ¼tfen 'GEMINI_API_KEY' sÄ±rrÄ±nÄ± Streamlit Secrets'a ekleyin.")
        return "API AnahtarÄ± bulunamadÄ±ÄŸÄ± iÃ§in cevap Ã¼retilemedi."

    # Sisteme verilecek talimat (System Instruction)
    system_instruction = (
        "Sen bir Erasmus programÄ± bilgi asistanÄ±sÄ±n. GÃ¶revin, SADECE saÄŸlanan 'CONTEXT' iÃ§erisindeki bilgileri kullanarak "
        "kullanÄ±cÄ±nÄ±n sorusunu doÄŸru ve yardÄ±mcÄ± bir ÅŸekilde cevaplamaktÄ±r. "
        "EÄŸer CONTEXT'te soruya dair bilgi yoksa, kibarca 'Bu konuda elimde kesin bir bilgi bulunmamaktadÄ±r.' diye yanÄ±tla. "
        "Harici bir bilgi kullanma."
    )
    
    # Modeli Ã§aÄŸÄ±rÄ±rken kullanÄ±lacak prompt
    prompt = f"""
    CONTEXT (Bilgi KaynaÄŸÄ±):
    ---
    {context}
    ---
    KULLANICI SORUSU:
    {query}
    """
    
    # Gemini modelini Ã§aÄŸÄ±rma (gemini-2.5-pro)
    response = client.models.generate_content(
        model=GENERATION_MODEL, 
        contents=prompt,
        config=genai.types.GenerateContentConfig(
            system_instruction=system_instruction
        )
    )
    
    return response.text

# --- 4. STREAMLIT ANA UYGULAMA MANTIÄI ---

# KaynaklarÄ± yÃ¼kle ve cache'le
df, embedding_model = load_resources()

st.title("ğŸ“ Erasmus RAG Chatbot Projesi")
st.markdown("Bu chatbot, **RAG (Retrieval Augmented Generation)** mimarisi kullanÄ±larak Erasmus veri setine dayalÄ± sorularÄ±nÄ±zÄ± yanÄ±tlar.")

if df is None or embedding_model is None:
    st.stop() # Kaynak yÃ¼klenemezse uygulamayÄ± durdur

# KullanÄ±cÄ±dan metin giriÅŸi alma
user_query = st.text_input("Erasmus programÄ± hakkÄ±nda sorunuz nedir?", placeholder="Ã–rn: Erasmus'a baÅŸvuru ÅŸartlarÄ± nelerdir?")

if user_query:
    with st.spinner(f"Bilgi AranÄ±yor ({TOP_K_DOCUMENTS} dokÃ¼man) ve Cevap Ãœretiliyor..."):
        
        # A. Retrieval (Geri Ã‡ekme)
        context_text = find_most_relevant_document(user_query, df, embedding_model, TOP_K_DOCUMENTS)
        
        # B. Generation (Cevap Ãœretme)
        final_answer = generate_rag_answer(user_query, context_text)
        
        # Sonucu gÃ¶sterme
        st.success("ğŸ¤– CHATBOT CEVABI:")
        st.info(final_answer)
        
        # KullanÄ±lan Context'i gÃ¶sterme
        with st.expander("KullanÄ±lan Bilgi KaynaÄŸÄ± (RAG Context)"):
            st.markdown(context_text)

st.divider()
st.caption("Ã‡Ã¶zÃ¼m Mimarisi: Sentence-Transformers (Embedding/Retrieval) + Gemini-2.5-Pro (Generation)")
